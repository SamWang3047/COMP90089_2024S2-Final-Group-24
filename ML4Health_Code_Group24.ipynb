{"cells":[{"cell_type":"markdown","metadata":{"id":"4_n_2IEQzd6U"},"source":["# Main Idea\n","\n","The main idea here is to identify **normal periods** (negative samples) and **acute hypotensive episodes** (positive samples). The plan is to use lab tests/measurements taken from several hours to half an hour before these periods (this window can be adjusted later for optimal performance) as features for prediction.\n","\n","## Key Measurements\n","The key measurements include:\n","- **Diastolic blood pressure** (220051)\n","- **Systolic blood pressure** (220050)\n","- **Heart rate** (220045)\n","- **SpO2** (220277)\n","- **MAP (Mean Arterial Pressure)** (220052)\n","- **Respiratory rate** (220210)\n","\n","Additionally, some general features like **age**, **gender**, etc., will be included to form the dataset for training and prediction.\n","\n","## Data Source and Processing\n","My data primarily comes from executing **SQL queries on Google BigQuery**. After that, the corresponding query results are downloaded as **CSV files**, which are then read into my code for use. If needed, we can modify the workflow to directly access the database and execute SQL queries in **Google Colab** to streamline the process later.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bc163611"},"outputs":[],"source":["from IPython.display import display\n","\n","import warnings\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"Lm0TJ5ObUb4v"},"source":["# Data Cleaning\n","\n","\n","\n","- Removed outliers from the dataset using Tukey's method and Modified Z-score method.\n","Replaced outliers with NaN and then dropped them from the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"24711ba9"},"outputs":[],"source":["#clean the \"all map for patient having less than 60.csv\" (used to extract samples) by removing outliers directly.\n","\n","import numpy as np\n","import pandas as pd\n","\n","# Tukey method to detect and replace outliers\n","def detect_outliers_tukey(df, value_column):\n","    Q1 = df[value_column].quantile(0.25)\n","    Q3 = df[value_column].quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower_bound = Q1 - 1.5 * IQR\n","    upper_bound = Q3 + 1.5 * IQR\n","\n","    # Detect outliers\n","    outliers = (df[value_column] < lower_bound) | (df[value_column] > upper_bound)\n","\n","    # Replace outliers with NaN\n","    df.loc[outliers, value_column] = np.nan\n","    return df\n","\n","# Modified Z-score method to detect and replace outliers\n","def detect_outliers_z(df, value_column):\n","    median = df[value_column].median()\n","    mad = np.median(np.abs(df[value_column] - median))\n","    modified_z_score = 0.6745 * (df[value_column] - median) / mad\n","\n","    # Detect outliers\n","    outliers = np.abs(modified_z_score) > 3.5\n","\n","    # Replace outliers with NaN\n","    df.loc[outliers, value_column] = np.nan\n","    return df\n","\n","# Detect and remove outliers using both methods\n","def clean_outliers(df, value_column='valuenum'):\n","    # First, use the Tukey method\n","    df = detect_outliers_tukey(df, value_column)\n","    # Then, use the modified Z-score method\n","    df = detect_outliers_z(df, value_column)\n","    # Remove rows with NaN values\n","    df = df.dropna(subset=[value_column])\n","    return df\n","\n","# Load data\n","df = pd.read_csv('all map for patient having less than 60.csv')\n","\n","# Clean outliers in the 'valuenum' column before filtering\n","df = clean_outliers(df, value_column='valuenum')"]},{"cell_type":"markdown","metadata":{"id":"cETkfSm7U7Fq"},"source":["# Extracting Positive Samples (Acute Hypotensive Episodes)\n","\n","\n","- Identified periods where patients experienced acute hypotensive episodes based on specific criteria.\n","- Filtered the dataset to include only these periods."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fce0f2b6"},"outputs":[],"source":["#extract positive samples(periods of acute hypotensive episodes)\n","#The criteria for selection are: if there are two consecutive records（labtest） with values less than 60 mmHg, and the time difference between them is greater than 30 minutes, then this period is considered as an acute hypotensive episode.\n","#Additionally, I need to check whether the record（labtest） prior to these two records were greater than 60 mmHg to ensure that the hypotensive episode started precisely at this period, and it's not part of an ongoing event.\n","\n","# Sort the dataframe by subject_id, hadm_id, stay_id, and charttime\n","df.sort_values(by=['subject_id', 'hadm_id','stay_id', 'charttime'], inplace=True)\n","\n","# Convert charttime to datetime format\n","df['charttime'] = pd.to_datetime(df['charttime'])\n","\n","# Calculate the time difference between the current and the next record, and create a new column for time difference\n","df['time_diff_next'] = df.groupby(['subject_id', 'hadm_id','stay_id'])['charttime'].shift(-1) - df['charttime']\n","df['time_diff'] = df.groupby(['subject_id', 'hadm_id','stay_id'])['charttime'].diff()\n","\n","# Columns that need to be checked for identical values\n","columns_to_check = ['subject_id', 'hadm_id', 'stay_id']\n","\n","# Filtering condition\n","condition = (\n","    ((df['valuenum'] < 60) &  # The current value is less than 60\n","    (df['valuenum'].shift(-1) < 60) &  # The next value is also less than 60\n","    (df['valuenum'].rolling(window=2, min_periods=2).min().shift(1) >= 60) &  # The minimum value in the 2-window rolling period before is greater than or equal to 60\n","    (df['time_diff_next'] > pd.Timedelta(minutes=30)) &  # The time difference between the current and next record is greater than 30 minutes\n","    (df['time_diff_next'] < pd.Timedelta(hours=12)) &  # The time difference between the current and next record is less than 12 hours\n","    ((df['charttime'] - df.groupby(['subject_id', 'hadm_id','stay_id'])['charttime'].shift(2)) >= pd.Timedelta(hours=1)) &  # Time difference from 2 records before is greater than or equal to 1 hour\n","    ((df['charttime'] - df.groupby(['subject_id', 'hadm_id','stay_id'])['charttime'].shift(2)) <= pd.Timedelta(hours=24)) &  # Time difference from 2 records before is less than or equal to 24 hours\n","    (df[columns_to_check].shift(1) == df[columns_to_check]).all(axis=1) &  # Ensure the values for subject_id, hadm_id, stay_id remain the same in previous records\n","    (df[columns_to_check].shift(2) == df[columns_to_check]).all(axis=1) &  # Ensure the values for subject_id, hadm_id, stay_id remain the same 2 records back\n","    (df[columns_to_check].shift(-1) == df[columns_to_check]).all(axis=1))   # Ensure the values for subject_id, hadm_id, stay_id remain the same in the next record\n",")\n","\n","# Filter the dataframe based on the above condition\n","filtered_df_1 = df[condition]\n","\n","# Display the filtered result\n","print(filtered_df_1)\n","\n","# Calculate the number of unique combinations of subject_id and hadm_id\n","unique_combinations_count = filtered_df_1.groupby(['subject_id', 'hadm_id']).ngroups\n","\n","# Display the result\n","#print(f\"Number of unique subject_id + hadm_id combinations: {unique_combinations_count}\")\n","display(len(filtered_df_1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"feadbaa5"},"outputs":[],"source":["#Simply check if the selected data is what I want\n","\n","i = df.index[df['charttime'] == '2144-01-27 20:02:00'].tolist()[0]\n","\n","\n","previous_row = df.iloc[i-3] if i > 0 else None\n","current_row = df.iloc[i]\n","next_row = df.iloc[i+1] if i < len(df)-1 else None\n","\n","\n","display(\"previous record:\")\n","display(previous_row)\n","\n","display(\"\\n current:\")\n","display(current_row)\n","\n","display(\"\\n next record:\")\n","display(next_row)"]},{"cell_type":"markdown","metadata":{"id":"Cj-nOg_ZVWMB"},"source":["# Extracting Negative Samples (Normal Periods)\n","\n","\n","- Identified normal periods where measurements were consistently above 60 mmHg.\n","- Filtered the dataset to include only these normal periods.\n","- Selected only one sample per day to reduce the number of negative samples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e693fe7f"},"outputs":[],"source":["# extract negative samples(normal period)\n","#If a record, including the previous and subsequent records, has values all greater than 60 mmHg, we can consider this period as a normal period.\n","\n","condition = (\n","    (df['valuenum'] >= 60) &\n","    (df['valuenum'].rolling(window=3, min_periods=3).min().shift(-1) >= 60) &\n","    (df['valuenum'].rolling(window=3, min_periods=3).min().shift(1) >= 60) &\n","    ((df['charttime'] - df.groupby(['subject_id', 'hadm_id','stay_id'])['charttime'].shift(3)) >= pd.Timedelta(hours=1.5)) &\n","    ((df['charttime'] - df.groupby(['subject_id', 'hadm_id','stay_id'])['charttime'].shift(3)) <= pd.Timedelta(hours=12)) &\n","    ((df.groupby(['subject_id', 'hadm_id','stay_id'])['charttime'].shift(-3))-df['charttime'] >= pd.Timedelta(hours=1.5)) &\n","    ((df.groupby(['subject_id', 'hadm_id','stay_id'])['charttime'].shift(-3))-df['charttime'] <= pd.Timedelta(hours=12)) &\n","    (df[columns_to_check].shift(1) == df[columns_to_check]).all(axis=1) &\n","    (df[columns_to_check].shift(2) == df[columns_to_check]).all(axis=1) &\n","    (df[columns_to_check].shift(3) == df[columns_to_check]).all(axis=1) &\n","    (df[columns_to_check].shift(-1) == df[columns_to_check]).all(axis=1) &\n","    (df[columns_to_check].shift(-2) == df[columns_to_check]).all(axis=1) &\n","    (df[columns_to_check].shift(-3) == df[columns_to_check]).all(axis=1)\n","\n",")\n","\n","\n","filtered_df_0 = df[condition]\n","\n","\n","print(filtered_df_0)\n","\n","\n","unique_combinations_count = filtered_df_0.groupby(['subject_id', 'hadm_id']).ngroups\n","\n","\n","#print(f\"Number of unique subject_id + hadm_id combinations:: {unique_combinations_count}\")\n","print(len(filtered_df_0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bc5b9d8e"},"outputs":[],"source":["# Since there are too many negative samples (normal periods), only one sample per day will be selected\n","filtered_df_0['date'] = filtered_df_0['charttime'].dt.date\n","\n","# Group by subject_id, hadm_id, stay_id, date and select the last record from each group\n","filtered_df_0_unique = filtered_df_0.groupby(['subject_id', 'hadm_id', 'stay_id', 'date']).last().reset_index()\n","\n","# View the result\n","print(filtered_df_0_unique)\n","\n","# Calculate the number of unique subject_id + hadm_id combinations\n","unique_combinations_count = filtered_df_0_unique.groupby(['subject_id', 'hadm_id']).ngroups\n","print(f\"Number of unique subject_id + hadm_id combinations: {unique_combinations_count}\")\n","print(len(filtered_df_0_unique))"]},{"cell_type":"markdown","metadata":{"id":"P02vdxq0VojL"},"source":["# Balancing the Dataset\n","\n","\n","- Added labels to the positive (1) and negative (0) samples.\n","- Combined the datasets and performed downsampling to create a balanced dataset with equal numbers of positive and negative samples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6c92c7f"},"outputs":[],"source":["# Add label to the samples\n","filtered_df_1['label'] = 1\n","filtered_df_0_unique['label'] = 0\n","\n","# Select the required columns\n","columns_to_keep = ['subject_id', 'hadm_id', 'stay_id', 'caregiver_id', 'charttime', 'label']\n","\n","# Concatenate the two DataFrames by columns\n","combined_df = pd.concat([filtered_df_1[columns_to_keep], filtered_df_0_unique[columns_to_keep]])\n","\n","# Reset the index\n","combined_df.reset_index(drop=True, inplace=True)\n","\n","# View the combined result\n","combined_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"197a901b"},"outputs":[],"source":["# Negative samples still far outnumber positive samples, so we perform downsampling\n","# To create balanced dataset with 8,000 samples (4,000 from each class)\n","\n","# Sample 4,000 samples from the data with label 0\n","df_label_0 = combined_df[combined_df['label'] == 0].sample(n=4000, random_state=42)\n","\n","# Sample 4,000 samples from the data with label 1\n","df_label_1 = combined_df[combined_df['label'] == 1].sample(n=4000, random_state=42)\n","\n","# Concatenate the two sampled datasets\n","combined_sampled_df = pd.concat([df_label_0, df_label_1])\n","\n","# Reset the index\n","combined_sampled_df.reset_index(drop=True, inplace=True)\n","\n","# View the result\n","print(\"Combined sampled dataset shape:\", combined_sampled_df.shape)\n","combined_sampled_df.head()"]},{"cell_type":"markdown","metadata":{"id":"ef069c77"},"source":["# Cleaning Feature Data (item3.csv)\n","\n","\n","- Cleaned the item3.csv file by detecting and handling outliers.\n","- Replaced outliers using interpolation for continuous data.\n","- Saved the cleaned data to new files for future use.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdc522f1"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","# Function to detect and replace outliers using Tukey's method\n","def detect_outliers_tukey(df, value_column):\n","   Q1 = df[value_column].quantile(0.25)\n","   Q3 = df[value_column].quantile(0.75)\n","   IQR = Q3 - Q1\n","   lower_bound = Q1 - 1.5 * IQR\n","   upper_bound = Q3 + 1.5 * IQR\n","\n","   # Detect outliers\n","   outliers = (df[value_column] < lower_bound) | (df[value_column] > upper_bound)\n","\n","   # Replace outliers with NaN\n","   df.loc[outliers, value_column] = np.nan\n","   return df\n","\n","# Function to detect and replace outliers using Modified Z-score method\n","def detect_outliers_z(df, value_column):\n","   median = df[value_column].median()\n","   mad = np.median(np.abs(df[value_column] - median))\n","   modified_z_score = 0.6745 * (df[value_column] - median) / mad\n","\n","   # Detect outliers\n","   outliers = np.abs(modified_z_score) > 3.5\n","\n","   # Replace outliers with NaN\n","   df.loc[outliers, value_column] = np.nan\n","   return df\n","\n","# Group by 'itemid' to handle different measurements separately\n","def clean_outliers_by_itemid(df, value_column='valuenum'):\n","   df_cleaned = pd.DataFrame()\n","\n","   # Process each itemid separately\n","   for itemid, group in df.groupby('itemid'):\n","       # Detect outliers using Tukey's method and Modified Z-score method\n","       group = detect_outliers_tukey(group, value_column)\n","       group = detect_outliers_z(group, value_column)\n","\n","       # Fill NaN values (outliers) using linear interpolation\n","       group[value_column] = group[value_column].interpolate(method='linear')\n","\n","       # Append cleaned group\n","       df_cleaned = pd.concat([df_cleaned, group], axis=0)\n","       print(\"1\")\n","\n","   return df_cleaned\n","\n","# Example usage:\n","# Load your dataset\n","items_df = pd.read_csv('item3.csv')\n","\n","# Clean outliers in your dataset\n","items_df_cleaned = clean_outliers_by_itemid(items_df)\n","\n","# Save the cleaned data to a new file if needed\n","items_df_cleaned.to_csv('cleaned_item3.csv', index=False)"]},{"cell_type":"code","source":["print(items_df.shape)\n","print(items_df_cleaned.shape)"],"metadata":{"id":"DIINDbL_PrZ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ae6adf5"},"outputs":[],"source":["#import numpy as np\n","#import pandas as pd\n","#\n","## Function to detect and replace outliers using Tukey's method\n","#def detect_outliers_tukey(df, value_column):\n","#    Q1 = df[value_column].quantile(0.25)\n","#    Q3 = df[value_column].quantile(0.75)\n","#    IQR = Q3 - Q1\n","#    lower_bound = Q1 - 1.5 * IQR\n","#    upper_bound = Q3 + 1.5 * IQR\n","#\n","#    # Detect outliers\n","#    outliers = (df[value_column] < lower_bound) | (df[value_column] > upper_bound)\n","#\n","#    # Replace outliers with NaN\n","#    df.loc[outliers, value_column] = np.nan\n","#    return df\n","#\n","## Function to detect and replace outliers using Modified Z-score method\n","#def detect_outliers_z(df, value_column):\n","#    median = df[value_column].median()\n","#    mad = np.median(np.abs(df[value_column] - median))\n","#    modified_z_score = 0.6745 * (df[value_column] - median) / mad\n","#\n","#    # Detect outliers\n","#    outliers = np.abs(modified_z_score) > 3.5\n","#\n","#    # Replace outliers with NaN\n","#    df.loc[outliers, value_column] = np.nan\n","#    return df\n","#\n","## Group by 'itemid' to handle different measurements separately\n","#def clean_outliers_by_itemid(df, value_column='valuenum'):\n","#    df_cleaned = pd.DataFrame()\n","#\n","#    # Process each itemid separately\n","#    for itemid, group in df.groupby('itemid'):\n","#        # Detect outliers using Tukey's method and Modified Z-score method\n","#        group = detect_outliers_tukey(group, value_column)\n","#        group = detect_outliers_z(group, value_column)\n","#\n","#        # Drop rows with NaN values (outliers)\n","#        group = group.dropna(subset=[value_column])\n","#\n","#        # Append cleaned group\n","#        df_cleaned = pd.concat([df_cleaned, group], axis=0)\n","#\n","#    return df_cleaned\n","#\n","## Example usage:\n","## Load your dataset\n","#items_df = pd.read_csv('item3.csv')\n","#\n","## Clean outliers in your dataset\n","#items_df_cleaned = clean_outliers_by_itemid(items_df)\n","#\n","## Save the cleaned data to a new file if needed\n","#items_df_cleaned.to_csv('cleaned_item3_2.csv', index=False)\n","#"]},{"cell_type":"markdown","metadata":{"id":"6uhurtTfWkK3"},"source":["# Feature Extraction\n","\n","\n","- Extracted statistical features (mean, max, min, median, etc.) for each measurement within a specified time window (from 6.5 to 0.5 hours before the sample time).\n","- Calculated 12 features consistent with the method used in the reference paper.\n","- Stored the features in a new DataFrame for merging."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"010a2f53"},"outputs":[],"source":[" from scipy.stats import skew, kurtosis\n","\n"," items_df = pd.read_csv('cleaned_item3.csv')\n","\n"," # First, convert charttime to datetime format\n"," combined_sampled_df['charttime'] = pd.to_datetime(combined_sampled_df['charttime'])\n"," items_df['charttime'] = pd.to_datetime(items_df['charttime'])\n","\n"," # Create an empty list to store the results\n"," new_features = []\n"," i = 0\n","\n"," # Iterate through each row in combined_df\n"," for _, row in combined_sampled_df.iterrows():\n","     subject_id = row['subject_id']\n","     hadm_id = row['hadm_id']\n","     stay_id = row['stay_id']\n","     charttime = row['charttime']\n","\n","     # Find matching records in items_df based on subject_id, hadm_id, and stay_id\n","     relevant_items = items_df[\n","         (items_df['subject_id'] == subject_id) &\n","         (items_df['hadm_id'] == hadm_id) &\n","         (items_df['stay_id'] == stay_id)\n","     ]\n","\n","     # Filter events within the time window (from 0.5 hours to 6.5 hours before the current record)\n","     time_window_items = relevant_items[\n","         (relevant_items['charttime'] >= charttime - pd.Timedelta(hours=6.5)) &\n","         (relevant_items['charttime'] <= charttime - pd.Timedelta(hours=0.5))\n","     ]\n","\n","     # Since multiple measurements may be taken within this time window, we choose to use statistics such as the mean, max, min, etc. (a total of 12 features),\n","     # This 12 features consistent with the method used in the reference paper.\n","     # Group by itemid and calculate statistics for valuenum (mean, max, min, etc.)\n","     grouped = time_window_items.groupby('itemid')['valuenum'].agg([\n","         'mean',                # Mean\n","         'max',                 # Maximum\n","         'min',                 # Minimum\n","         'median',              # Median\n","         'std',                 # Standard Deviation\n","         ('skewness', skew),    # Skewness (using scipy.stats.skew)\n","         ('kurtosis', kurtosis),# Kurtosis (using scipy.stats.kurtosis)\n","         ('q75', lambda x: np.percentile(x, 75)),  # 75th percentile\n","         ('q25', lambda x: np.percentile(x, 25)),  # 25th percentile\n","         ('mad', lambda x: np.mean(np.abs(x - np.mean(x)))),  # Mean Absolute Deviation\n","         ('range', lambda x: np.max(x) - np.min(x)),  # Range\n","         'var'                  # Variance\n","     ])\n","\n","     # Store the result as a dictionary for easier merging with combined_df\n","     grouped_dict = grouped.to_dict('index')\n","\n","     # Add to the new_features list to be merged later\n","     new_features.append(grouped_dict)\n","\n","     # Print progress\n","     i += 1\n","     print(i)\n","\n"," # Convert the new features to a DataFrame and merge with combined_df\n"," new_features_df = pd.DataFrame(new_features)\n"," final_df = pd.concat([combined_sampled_df.reset_index(drop=True), new_features_df.reset_index(drop=True)], axis=1)"]},{"cell_type":"markdown","metadata":{"id":"BYfCfPbRW1dE"},"source":["# Adding Additional Features (Vasopressin and Ventilation Usage)\n","\n","\n","- Added binary features indicating whether vasopressin or ventilation was used within the specified time window before the sample time.\n","- Although initial tests showed these features might not significantly affect results, they were included for completeness."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6344d3b"},"outputs":[],"source":["# Add vasopressin and ventilation usage information as binary features (1 if used, 0 if not)\n","# Testing revealed that these two features do not significantly affect the results, so they might be discarded.\n","\n","vasopressin_df = pd.read_csv('vasopressin.csv')\n","ventilation_df = pd.read_csv('ventilation.csv')\n","\n","vasopressin_df['starttime'] = pd.to_datetime(vasopressin_df['starttime'])\n","vasopressin_df['endtime'] = pd.to_datetime(vasopressin_df['endtime'])\n","ventilation_df['starttime'] = pd.to_datetime(ventilation_df['starttime'])\n","ventilation_df['endtime'] = pd.to_datetime(ventilation_df['endtime'])\n","\n","\n","\n","# Define a function to check if vasopressin or ventilation was given in the required time window\n","def check_treatment(row):\n","    charttime = row['charttime']\n","    stay_id = row['stay_id']\n","\n","    # Time window: charttime - 1.5 hours to charttime - 0.5 hours\n","    time_start = charttime - pd.Timedelta(hours=6.5)\n","    time_end = charttime - pd.Timedelta(hours=0.5)\n","\n","    # Check for vasopressin\n","    vasopressin_given = vasopressin_df[\n","        (vasopressin_df['stay_id'] == stay_id) &\n","        (vasopressin_df['starttime'] >= time_start) &\n","        (vasopressin_df['starttime'] <= time_end)\n","    ]\n","\n","    # Check for ventilation\n","    ventilation_given = ventilation_df[\n","        (ventilation_df['stay_id'] == stay_id) &\n","        (ventilation_df['starttime'] >= time_start) &\n","        (ventilation_df['starttime'] <= time_end)\n","    ]\n","\n","    # Assign 1 if given, 0 otherwise\n","    row['vasopressin'] = 1 if not vasopressin_given.empty else 0\n","    row['ventilation'] = 1 if not ventilation_given.empty else 0\n","\n","    return row\n","\n","# Apply the function to every row in final_df_cleaned\n","final_df = final_df.apply(check_treatment, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"-KoMhx8uXDL_"},"source":["# Adding General Features (Age, Gender, APSIII, SOFA)\n","\n","\n","- Merged additional patient data such as age, gender, APSIII scores, and SOFA scores into the main dataset.\n","- This provided more context and potential predictive power for the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fc36c627"},"outputs":[],"source":["#add general features like age, gender...\n","\n","apsiii_df = pd.read_csv('apsiii.csv')\n","age_df= pd.read_csv('age.csv')\n","gender_df= pd.read_csv('gender.csv')\n","sofa_df= pd.read_csv('sofa.csv')\n","\n","final_df = pd.merge(final_df, apsiii_df, on=['subject_id', 'hadm_id', 'stay_id'], how='left')\n","final_df = pd.merge(final_df, age_df, on=['subject_id', 'hadm_id'], how='left')\n","final_df = pd.merge(final_df, gender_df, on=['subject_id'], how='left')\n","final_df = pd.merge(final_df, sofa_df, on=['subject_id', 'hadm_id', 'stay_id'], how='left')\n","\n","# show results\n","display(final_df)"]},{"cell_type":"markdown","metadata":{"id":"zj-npaVkXQ3f"},"source":["# Preparing the Final Data for Modeling\n","What We Did:\n","\n","- Dropped rows with null values to ensure data quality.\n","- Converted categorical variables (e.g., gender) into numerical format suitable for modeling."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32a33236"},"outputs":[],"source":["# drop null values\n","final_df_cleaned = final_df.dropna()\n","\n","# Convert the values in the gender column: \"male\" to 1 and \"female\" to 0\n","final_df_cleaned['gender'] = final_df_cleaned['gender'].map({'M': 1, 'F': 0})\n","\n","\n","display(final_df_cleaned)"]},{"cell_type":"markdown","metadata":{"id":"pUs3yH0nXepd"},"source":["# Extracting Statistical Features for Each Lab Test\n","\n","\n","- Extracted 12 statistical features for each lab test measurement.\n","- These features include mean, max, min, median, standard deviation, skewness, kurtosis, percentiles, etc.\n","- Removed the original nested data structures after extracting the features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ce3479c6"},"outputs":[],"source":["# Extract 12 feature values for each lab test\n","#diastolic-220051  systolic-220050   Heart Rate-220045  SpO2-220277   map-220052   Respiratory Rate-220210\n","def extract_features(item_series):\n","    return pd.Series({\n","        'mean': item_series.get('mean'),\n","        'max': item_series.get('max'),\n","        'min': item_series.get('min'),\n","        'median': item_series.get('median'),\n","        'std': item_series.get('std'),\n","        'skewness': item_series.get('skewness'),\n","        'kurtosis': item_series.get('kurtosis'),\n","        'q75': item_series.get('q75'),\n","        'q25': item_series.get('q25'),\n","        'mad': item_series.get('mad'),\n","        'range': item_series.get('range'),\n","        'var': item_series.get('var')\n","    })\n","\n","\n","final_df_cleaned[['220051_mean', '220051_max', '220051_min', '220051_median', '220051_std', '220051_skewness',\n","                  '220051_kurtosis', '220051_q75', '220051_q25', '220051_mad', '220051_range', '220051_var']] = \\\n","    final_df_cleaned[220051].apply(extract_features)\n","\n","\n","final_df_cleaned[['220050_mean', '220050_max', '220050_min', '220050_median', '220050_std', '220050_skewness',\n","                  '220050_kurtosis', '220050_q75', '220050_q25', '220050_mad', '220050_range', '220050_var']] = \\\n","    final_df_cleaned[220050].apply(extract_features)\n","\n","\n","final_df_cleaned[['220045_mean', '220045_max', '220045_min', '220045_median', '220045_std', '220045_skewness',\n","                  '220045_kurtosis', '220045_q75', '220045_q25', '220045_mad', '220045_range', '220045_var']] = \\\n","    final_df_cleaned[220045].apply(extract_features)\n","\n","\n","final_df_cleaned[['220277_mean', '220277_max', '220277_min', '220277_median', '220277_std', '220277_skewness',\n","                  '220277_kurtosis', '220277_q75', '220277_q25', '220277_mad', '220277_range', '220277_var']] = \\\n","    final_df_cleaned[220277].apply(extract_features)\n","\n","\n","final_df_cleaned[['220052_mean', '220052_max', '220052_min', '220052_median', '220052_std', '220052_skewness',\n","                  '220052_kurtosis', '220052_q75', '220052_q25', '220052_mad', '220052_range', '220052_var']] = \\\n","    final_df_cleaned[220052].apply(extract_features)\n","\n","final_df_cleaned[['220210_mean', '220210_max', '220210_min', '220210_median', '220210_std', '220210_skewness',\n","                  '220210_kurtosis', '220210_q75', '220210_q25', '220210_mad', '220210_range', '220210_var']] = \\\n","    final_df_cleaned[220210].apply(extract_features)\n","\n","\n","final_df_cleaned.drop(columns=[220051, 220050, 220045, 220277, 220052, 220210], inplace=True)\n","\n","\n","#print(final_df_cleaned)\n","\n","final_df_cleaned = final_df_cleaned.dropna()\n","\n","\n","display(final_df_cleaned)"]},{"cell_type":"markdown","source":["#Prepare Data"],"metadata":{"id":"CU5ck0djO1Nc"}},{"cell_type":"code","source":["# Exclude non-feature columns\n","non_feature_columns = ['label', 'subject_id', 'hadm_id', 'stay_id', 'caregiver_id', 'charttime','anchor_age']\n","\n","# Features (X) and target (y)\n","X = final_df_cleaned.drop(columns=non_feature_columns)\n","y = final_df_cleaned['label']\n","\n","# Ensure there are no missing values\n","X = X.dropna()\n","y = y.loc[X.index]\n","\n","print(\"Features shape:\", X.shape)\n","print(\"Target shape:\", y.shape)"],"metadata":{"id":"SgtilVoGO4qr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature Selection using ReliefF\n","\n","- apply the ReliefF algorithm to rank the features\n","- fs_relief.feature_importances_ contains the importance scores assigned to each feature by ReliefF.\n","- create a DataFrame to map each feature to its score.\n","- sort the features in descending order of importance."],"metadata":{"id":"XLUOP4spQTEx"}},{"cell_type":"code","source":["!pip install skrebate"],"metadata":{"id":"3Q8hGh8KPop0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from skrebate import ReliefF\n","\n","# Initialize ReliefF\n","fs_relief = ReliefF(n_neighbors=100, n_features_to_select=X.shape[1])\n","\n","# Fit the model\n","fs_relief.fit(X.values, y.values)\n","\n","# Get feature scores\n","relief_scores = fs_relief.feature_importances_\n","\n","# Create a DataFrame for easy viewing\n","relief_feature_scores = pd.DataFrame({'Feature': X.columns, 'ReliefF_Score': relief_scores})\n","\n","# Sort the features by score\n","relief_feature_scores.sort_values(by='ReliefF_Score', ascending=False, inplace=True)\n","\n","# Display the top features\n","print(\"Top features according to ReliefF:\")\n","print(relief_feature_scores.head(20))"],"metadata":{"id":"VXxWLWj2PtFn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature Selection using Mutual Information"],"metadata":{"id":"8A1YkagYQo3Q"}},{"cell_type":"code","source":["from sklearn.feature_selection import mutual_info_classif\n","\n","# Compute mutual information scores\n","mi_scores = mutual_info_classif(X, y)\n","\n","# Create a DataFrame for easy viewing\n","mi_feature_scores = pd.DataFrame({'Feature': X.columns, 'MI_Score': mi_scores})\n","\n","# Sort the features by score\n","mi_feature_scores.sort_values(by='MI_Score', ascending=False, inplace=True)\n","\n","# Display the top features\n","print(\"Top features according to Mutual Information:\")\n","print(mi_feature_scores.head(20))"],"metadata":{"id":"ltYJyhv7fHZg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feature Selection using Gini Index\n","\n","use a Decision Tree Classifier to compute feature importances based on the Gini Index.\n","\n","- The Decision Tree Classifier computes feature importances based on how much each feature decreases the impurity (Gini Index) in the tree."],"metadata":{"id":"MK7Dfm6URCSi"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","\n","# Initialize Decision Tree Classifier\n","dt_clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n","\n","# Fit the model\n","dt_clf.fit(X, y)\n","\n","# Get feature importances\n","gini_importances = dt_clf.feature_importances_\n","\n","# Create a DataFrame for easy viewing\n","gini_feature_importances = pd.DataFrame({'Feature': X.columns, 'Gini_Importance': gini_importances})\n","\n","# Sort the features by importance\n","gini_feature_importances.sort_values(by='Gini_Importance', ascending=False, inplace=True)\n","\n","# Display the top features\n","print(\"Top features according to Gini Importance:\")\n","print(gini_feature_importances.head(20))"],"metadata":{"id":"QCIwMrNnRId4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Combine Feature Scores"],"metadata":{"id":"5IfGfgFFakdf"}},{"cell_type":"code","source":["# Merge the feature scores on the 'Feature' column\n","feature_scores = relief_feature_scores.merge(mi_feature_scores, on='Feature')\n","feature_scores = feature_scores.merge(gini_feature_importances, on='Feature')\n","\n","print(\"Combined Feature Scores:\")\n","print(feature_scores.head())"],"metadata":{"id":"pSst3Mdqf3eg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["normalize the scores to ensure they are on the same scale before combining them."],"metadata":{"id":"k9EnMx1AgNgX"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","feature_scores[['ReliefF_Score', 'MI_Score', 'Gini_Importance']] = scaler.fit_transform(\n","    feature_scores[['ReliefF_Score', 'MI_Score', 'Gini_Importance']])\n","\n","print(\"Normalized Feature Scores:\")\n","print(feature_scores.head())"],"metadata":{"id":"bYNA9r8lf_75"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["compute the combined score by summing the normalized scores."],"metadata":{"id":"aeRDPij7gW6M"}},{"cell_type":"code","source":["# Compute the combined score\n","feature_scores['Combined_Score'] = (feature_scores['ReliefF_Score'] +\n","                                    feature_scores['MI_Score'] +\n","                                    feature_scores['Gini_Importance'])\n","\n","# Rank features based on the combined score\n","feature_scores.sort_values(by='Combined_Score', ascending=False, inplace=True)\n","\n","# Reset index\n","feature_scores.reset_index(drop=True, inplace=True)\n","\n","\n","print(\"Top features according to Combined Score:\")\n","print(feature_scores[['Feature', 'Combined_Score']].head(20))"],"metadata":{"id":"3WSrCVjMgYTg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the number of features to select (e.g., top 39%)\n","num_features = int(len(feature_scores) * 0.61)\n","\n","print(f\"Number of features to select: {num_features}\")"],"metadata":{"id":"K--evwpqguby"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the list of selected features\n","selected_features = feature_scores['Feature'].values[:num_features]\n","\n","print(\"Selected features:\")\n","print(selected_features)"],"metadata":{"id":"Rc4gMFy0gzRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a new DataFrame with selected features\n","X_selected = X[selected_features]\n","\n","print(\"Shape of X_selected:\", X_selected.shape)"],"metadata":{"id":"yggLGSdHg6BI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Split the Data into Training and Testing Sets"],"metadata":{"id":"MCVC09P8g-sD"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Stratify ensures the class distribution remains balanced\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_selected, y, test_size=0.3, random_state=42, stratify=y)\n","\n","print(\"Training set size:\", X_train.shape)\n","print(\"Test set size:\", X_test.shape)"],"metadata":{"id":"SWWE_up_hBuN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train and Evaluate Models"],"metadata":{"id":"9s2Ohw_YhIAR"}},{"cell_type":"markdown","source":["## Logistic Regression"],"metadata":{"id":"ZAAo1o-EhZgV"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","\n","# Initialize the model\n","lr_clf = LogisticRegression(max_iter=1000, random_state=42)\n","\n","# Train the model\n","lr_clf.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred_lr = lr_clf.predict(X_test)\n","y_pred_prob_lr = lr_clf.predict_proba(X_test)[:, 1]\n","\n","# Evaluate the model\n","accuracy_lr = accuracy_score(y_test, y_pred_lr)\n","precision_lr = precision_score(y_test, y_pred_lr)\n","recall_lr = recall_score(y_test, y_pred_lr)\n","f1_lr = f1_score(y_test, y_pred_lr)\n","auc_lr = roc_auc_score(y_test, y_pred_prob_lr)\n","\n","print(\"Logistic Regression Performance:\")\n","print(f'Accuracy: {accuracy_lr:.3f}')\n","print(f'Precision: {precision_lr:.3f}')\n","print(f'Recall: {recall_lr:.3f}')\n","print(f'F1 Score: {f1_lr:.3f}')\n","print(f'AUC: {auc_lr:.3f}')"],"metadata":{"id":"bYUPIz4lhT3d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Random Forest"],"metadata":{"id":"Xx88SctThbdm"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Initialize the model\n","rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n","\n","# Train the model\n","rf_clf.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred_rf = rf_clf.predict(X_test)\n","y_pred_prob_rf = rf_clf.predict_proba(X_test)[:, 1]\n","\n","# Evaluate the model\n","accuracy_rf = accuracy_score(y_test, y_pred_rf)\n","precision_rf = precision_score(y_test, y_pred_rf)\n","recall_rf = recall_score(y_test, y_pred_rf)\n","f1_rf = f1_score(y_test, y_pred_rf)\n","auc_rf = roc_auc_score(y_test, y_pred_prob_rf)\n","\n","print(\"Random Forest Performance:\")\n","print(f'Accuracy: {accuracy_rf:.3f}')\n","print(f'Precision: {precision_rf:.3f}')\n","print(f'Recall: {recall_rf:.3f}')\n","print(f'F1 Score: {f1_rf:.3f}')\n","print(f'AUC: {auc_rf:.3f}')"],"metadata":{"id":"D5fmi4eBhf76"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## XGBoost"],"metadata":{"id":"aeppFiUOku4V"}},{"cell_type":"code","source":["import xgboost as xgb\n","\n","# Initialize the model\n","xgb_clf = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n","\n","# Train the model\n","xgb_clf.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred_xgb = xgb_clf.predict(X_test)\n","y_pred_prob_xgb = xgb_clf.predict_proba(X_test)[:, 1]\n","\n","# Evaluate the model\n","accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n","precision_xgb = precision_score(y_test, y_pred_xgb)\n","recall_xgb = recall_score(y_test, y_pred_xgb)\n","f1_xgb = f1_score(y_test, y_pred_xgb)\n","auc_xgb = roc_auc_score(y_test, y_pred_prob_xgb)\n","\n","print(\"XGBoost Performance:\")\n","print(f'Accuracy: {accuracy_xgb:.3f}')\n","print(f'Precision: {precision_xgb:.3f}')\n","print(f'Recall: {recall_xgb:.3f}')\n","print(f'F1 Score: {f1_xgb:.3f}')\n","print(f'AUC: {auc_xgb:.3f}')"],"metadata":{"id":"hWjZKVf5hkTt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## AdaBoost"],"metadata":{"id":"L-FD_jWYkniQ"}},{"cell_type":"code","source":["from sklearn.ensemble import AdaBoostClassifier\n","\n","# Initialize the model\n","ada_clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n","\n","# Train the model\n","ada_clf.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred_ada = ada_clf.predict(X_test)\n","y_pred_prob_ada = ada_clf.predict_proba(X_test)[:, 1]\n","\n","# Evaluate the model\n","accuracy_ada = accuracy_score(y_test, y_pred_ada)\n","precision_ada = precision_score(y_test, y_pred_ada)\n","recall_ada = recall_score(y_test, y_pred_ada)\n","f1_ada = f1_score(y_test, y_pred_ada)\n","auc_ada = roc_auc_score(y_test, y_pred_prob_ada)\n","\n","print(\"AdaBoost Performance:\")\n","print(f'Accuracy: {accuracy_ada:.6f}')\n","print(f'Precision: {precision_ada:.6f}')\n","print(f'Recall: {recall_ada:.6f}')\n","print(f'F1 Score: {f1_ada:.6f}')\n","print(f'AUC: {auc_ada:.6f}')"],"metadata":{"id":"tU58OfbykkXS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Support Vector Machine (SVM)"],"metadata":{"id":"ACnsxDiUk4Jm"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","\n","# Initialize the model\n","svm_clf = SVC(probability=True, random_state=42)\n","\n","# Train the model\n","svm_clf.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred_svm = svm_clf.predict(X_test)\n","y_pred_prob_svm = svm_clf.predict_proba(X_test)[:, 1]\n","\n","# Evaluate the model\n","accuracy_svm = accuracy_score(y_test, y_pred_svm)\n","precision_svm = precision_score(y_test, y_pred_svm)\n","recall_svm = recall_score(y_test, y_pred_svm)\n","f1_svm = f1_score(y_test, y_pred_svm)\n","auc_svm = roc_auc_score(y_test, y_pred_prob_svm)\n","\n","print(\"SVM Performance:\")\n","print(f'Accuracy: {accuracy_svm:.6f}')\n","print(f'Precision: {precision_svm:.6f}')\n","print(f'Recall: {recall_svm:.6f}')\n","print(f'F1 Score: {f1_svm:.6f}')\n","print(f'AUC: {auc_svm:.6f}')"],"metadata":{"id":"5AyF4lglk1pG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Gradient Boosting Decision Trees (GBDT)"],"metadata":{"id":"WZ6fp3uWk8Ck"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingClassifier\n","\n","# Initialize the model\n","gbdt_clf = GradientBoostingClassifier(random_state=42)\n","\n","# Train the model\n","gbdt_clf.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred_gbdt = gbdt_clf.predict(X_test)\n","y_pred_prob_gbdt = gbdt_clf.predict_proba(X_test)[:, 1]\n","\n","# Evaluate the model\n","accuracy_gbdt = accuracy_score(y_test, y_pred_gbdt)\n","precision_gbdt = precision_score(y_test, y_pred_gbdt)\n","recall_gbdt = recall_score(y_test, y_pred_gbdt)\n","f1_gbdt = f1_score(y_test, y_pred_gbdt)\n","auc_gbdt = roc_auc_score(y_test, y_pred_prob_gbdt)\n","\n","print(\"GBDT Performance:\")\n","print(f'Accuracy: {accuracy_gbdt:.6f}')\n","print(f'Precision: {precision_gbdt:.6f}')\n","print(f'Recall: {recall_gbdt:.6f}')\n","print(f'F1 Score: {f1_gbdt:.6f}')\n","print(f'AUC: {auc_gbdt:.6f}')"],"metadata":{"id":"kl4OV4rIk-SS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create a DataFrame for the New Models\n"],"metadata":{"id":"AB6Pt4cHlMRS"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Create a dictionary with model names and their corresponding performance metrics\n","performance_data = {\n","    'Model': [\n","        'Logistic Regression',\n","        'Random Forest',\n","        'XGBoost',\n","        'AdaBoost',\n","        'SVM',\n","        'GBDT'\n","    ],\n","    'Accuracy': [\n","        accuracy_lr,\n","        accuracy_rf,\n","        accuracy_xgb,\n","        accuracy_ada,\n","        accuracy_svm,\n","        accuracy_gbdt\n","    ],\n","    'Precision': [\n","        precision_lr,\n","        precision_rf,\n","        precision_xgb,\n","        precision_ada,\n","        precision_svm,\n","        precision_gbdt\n","    ],\n","    'Recall': [\n","        recall_lr,\n","        recall_rf,\n","        recall_xgb,\n","        recall_ada,\n","        recall_svm,\n","        recall_gbdt\n","    ],\n","\n","    'F1 Score': [f1_lr,f1_rf, f1_xgb, f1_ada,f1_svm, f1_gbdt\n","    ],\n","\n","    'AUC': [ auc_lr,auc_rf, auc_xgb, auc_ada, auc_svm, auc_gbdt\n","    ]\n","}\n","\n","# Create the DataFrame\n","performance_df = pd.DataFrame(performance_data)\n","\n","# Display the performance comparison table\n","print(\"\\nModel Performance Comparison:\")\n","print(performance_df)"],"metadata":{"id":"OFSEYkeslw6O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Required libraries\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n","from sklearn.svm import SVC\n","import xgboost as xgb\n","\n","# Evaluate models before feature reduction\n","X_train_full, X_test_full, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42, stratify=y)\n","\n","# Train and evaluate models on full data before feature reduction\n","# Logistic Regression\n","lr_full = LogisticRegression(max_iter=1000, random_state=42)\n","lr_full.fit(X_train_full, y_train)\n","y_pred_lr_full = lr_full.predict(X_test_full)\n","y_pred_prob_lr_full = lr_full.predict_proba(X_test_full)[:, 1]\n","\n","# Random Forest\n","rf_full = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_full.fit(X_train_full, y_train)\n","y_pred_rf_full = rf_full.predict(X_test_full)\n","y_pred_prob_rf_full = rf_full.predict_proba(X_test_full)[:, 1]\n","\n","# XGBoost\n","xgb_full = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n","xgb_full.fit(X_train_full, y_train)\n","y_pred_xgb_full = xgb_full.predict(X_test_full)\n","y_pred_prob_xgb_full = xgb_full.predict_proba(X_test_full)[:, 1]\n","\n","# AdaBoost\n","ada_full = AdaBoostClassifier(n_estimators=100, random_state=42)\n","ada_full.fit(X_train_full, y_train)\n","y_pred_ada_full = ada_full.predict(X_test_full)\n","y_pred_prob_ada_full = ada_full.predict_proba(X_test_full)[:, 1]\n","\n","# SVM\n","svm_full = SVC(probability=True, random_state=42)\n","svm_full.fit(X_train_full, y_train)\n","y_pred_svm_full = svm_full.predict(X_test_full)\n","y_pred_prob_svm_full = svm_full.predict_proba(X_test_full)[:, 1]\n","\n","# GBDT\n","gbdt_full = GradientBoostingClassifier(random_state=42)\n","gbdt_full.fit(X_train_full, y_train)\n","y_pred_gbdt_full = gbdt_full.predict(X_test_full)\n","y_pred_prob_gbdt_full = gbdt_full.predict_proba(X_test_full)[:, 1]\n","\n","# Calculate metrics for each model before feature reduction\n","# Logistic Regression\n","accuracy_lr_full = accuracy_score(y_test, y_pred_lr_full)\n","precision_lr_full = precision_score(y_test, y_pred_lr_full)\n","recall_lr_full = recall_score(y_test, y_pred_lr_full)\n","f1_lr_full = f1_score(y_test, y_pred_lr_full)\n","auc_lr_full = roc_auc_score(y_test, y_pred_prob_lr_full)\n","\n","# Random Forest\n","accuracy_rf_full = accuracy_score(y_test, y_pred_rf_full)\n","precision_rf_full = precision_score(y_test, y_pred_rf_full)\n","recall_rf_full = recall_score(y_test, y_pred_rf_full)\n","f1_rf_full = f1_score(y_test, y_pred_rf_full)\n","auc_rf_full = roc_auc_score(y_test, y_pred_prob_rf_full)\n","\n","# XGBoost\n","accuracy_xgb_full = accuracy_score(y_test, y_pred_xgb_full)\n","precision_xgb_full = precision_score(y_test, y_pred_xgb_full)\n","recall_xgb_full = recall_score(y_test, y_pred_xgb_full)\n","f1_xgb_full = f1_score(y_test, y_pred_xgb_full)\n","auc_xgb_full = roc_auc_score(y_test, y_pred_prob_xgb_full)\n","\n","# AdaBoost\n","accuracy_ada_full = accuracy_score(y_test, y_pred_ada_full)\n","precision_ada_full = precision_score(y_test, y_pred_ada_full)\n","recall_ada_full = recall_score(y_test, y_pred_ada_full)\n","f1_ada_full = f1_score(y_test, y_pred_ada_full)\n","auc_ada_full = roc_auc_score(y_test, y_pred_prob_ada_full)\n","\n","# SVM\n","accuracy_svm_full = accuracy_score(y_test, y_pred_svm_full)\n","precision_svm_full = precision_score(y_test, y_pred_svm_full)\n","recall_svm_full = recall_score(y_test, y_pred_svm_full)\n","f1_svm_full = f1_score(y_test, y_pred_svm_full)\n","auc_svm_full = roc_auc_score(y_test, y_pred_prob_svm_full)\n","\n","# GBDT\n","accuracy_gbdt_full = accuracy_score(y_test, y_pred_gbdt_full)\n","precision_gbdt_full = precision_score(y_test, y_pred_gbdt_full)\n","recall_gbdt_full = recall_score(y_test, y_pred_gbdt_full)\n","f1_gbdt_full = f1_score(y_test, y_pred_gbdt_full)\n","auc_gbdt_full = roc_auc_score(y_test, y_pred_prob_gbdt_full)\n","\n","# Print formatted comparison table\n","print(\"Comparison of the prediction performance before and after feature reduction.\")\n","print(\"-\" * 100)\n","print(\"         Before feature reduction                           After feature reduction\")\n","print(\"Methods  Num   PRE    REC    ACC    F1     AUC     Num     PRE      REC    ACC    F1     AUC\")\n","print(\"-\" * 100)\n","\n","# Define feature counts\n","total_features = X.shape[1]  # Total features before reduction\n","selected_features = X_selected.shape[1]  # Total features after reduction\n","\n","# Print data for each model\n","methods = ['GBDT', 'SVM', 'LR', 'XGB', 'RF', 'AdaBoost']\n","for method in methods:\n","    # Print model name\n","    print(f\"{method:<8}\", end=\"\")\n","\n","    # Print metrics before feature reduction\n","    print(f\"{total_features:<6}\", end=\"\")\n","\n","    if method == 'GBDT':\n","        print(f\"{precision_gbdt_full:.3f}  {recall_gbdt_full:.3f}  {accuracy_gbdt_full:.3f}  {f1_gbdt_full:.3f}  {auc_gbdt_full:.3f}     \", end=\"\")\n","        print(f\"{selected_features:<6} {precision_gbdt:.3f}  {recall_gbdt:.3f}  {accuracy_gbdt:.3f}  {f1_gbdt:.3f}  {auc_gbdt:.3f}\")\n","    elif method == 'SVM':\n","        print(f\"{precision_svm_full:.3f}  {recall_svm_full:.3f}  {accuracy_svm_full:.3f}  {f1_svm_full:.3f}  {auc_svm_full:.3f}     \", end=\"\")\n","        print(f\"{selected_features:<6} {precision_svm:.3f}  {recall_svm:.3f}  {accuracy_svm:.3f}  {f1_svm:.3f}  {auc_svm:.3f}\")\n","    elif method == 'LR':\n","        print(f\"{precision_lr_full:.3f}  {recall_lr_full:.3f}  {accuracy_lr_full:.3f}  {f1_lr_full:.3f}  {auc_lr_full:.3f}     \", end=\"\")\n","        print(f\"{selected_features:<6} {precision_lr:.3f}  {recall_lr:.3f}  {accuracy_lr:.3f}  {f1_lr:.3f}  {auc_lr:.3f}\")\n","    elif method == 'XGB':\n","        print(f\"{precision_xgb_full:.3f}  {recall_xgb_full:.3f}  {accuracy_xgb_full:.3f}  {f1_xgb_full:.3f}  {auc_xgb_full:.3f}     \", end=\"\")\n","        print(f\"{selected_features:<6} {precision_xgb:.3f}  {recall_xgb:.3f}  {accuracy_xgb:.3f}  {f1_xgb:.3f}  {auc_xgb:.3f}\")\n","    elif method == 'RF':\n","        print(f\"{precision_rf_full:.3f}  {recall_rf_full:.3f}  {accuracy_rf_full:.3f}  {f1_rf_full:.3f}  {auc_rf_full:.3f}     \", end=\"\")\n","        print(f\"{selected_features:<6} {precision_rf:.3f}  {recall_rf:.3f}  {accuracy_rf:.3f}  {f1_rf:.3f}  {auc_rf:.3f}\")\n","    elif method == 'AdaBoost':\n","        print(f\"{precision_ada_full:.3f}  {recall_ada_full:.3f}  {accuracy_ada_full:.3f}  {f1_ada_full:.3f}  {auc_ada_full:.3f}     \", end=\"\")\n","        print(f\"{selected_features:<6} {precision_ada:.3f}  {recall_ada:.3f}  {accuracy_ada:.3f}  {f1_ada:.3f}  {auc_ada:.3f}\")\n","\n","print(\"-\" * 100)"],"metadata":{"id":"DXrcWROJRg2E"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1d68b739"},"outputs":[],"source":["# # Further attempt with ensemble learning to see if there is any improvement\n","\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.preprocessing import StandardScaler\n","# from sklearn.linear_model import LogisticRegression\n","# from sklearn.neural_network import MLPClassifier\n","# from sklearn.naive_bayes import GaussianNB\n","# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, StackingClassifier\n","# from sklearn.tree import DecisionTreeClassifier\n","# from sklearn.svm import SVC\n","# from sklearn.neighbors import KNeighborsClassifier\n","# from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n","# import xgboost as xgb\n","\n","# # Prepare the feature columns\n","# features = final_df_cleaned[[\n","#     '220051_mean', '220051_max', '220051_min', '220051_median', '220051_std',\n","#     '220051_skewness', '220051_kurtosis', '220051_q75', '220051_q25',\n","#     '220051_mad', '220051_range', '220051_var',\n","\n","#     '220050_mean', '220050_max', '220050_min', '220050_median', '220050_std',\n","#     '220050_skewness', '220050_kurtosis', '220050_q75', '220050_q25',\n","#     '220050_mad', '220050_range', '220050_var',\n","\n","#     '220045_mean', '220045_max', '220045_min', '220045_median', '220045_std',\n","#     '220045_skewness', '220045_kurtosis', '220045_q75', '220045_q25',\n","#     '220045_mad', '220045_range', '220045_var',\n","\n","#     '220277_mean', '220277_max', '220277_min', '220277_median', '220277_std',\n","#     '220277_skewness', '220277_kurtosis', '220277_q75', '220277_q25',\n","#     '220277_mad', '220277_range', '220277_var',\n","\n","#     '220052_mean', '220052_max', '220052_min', '220052_median', '220052_std',\n","#     '220052_skewness', '220052_kurtosis', '220052_q75', '220052_q25',\n","#     '220052_mad', '220052_range', '220052_var',\n","\n","#     '220210_mean', '220210_max', '220210_min', '220210_median', '220210_std',\n","#     '220210_skewness', '220210_kurtosis', '220210_q75', '220210_q25',\n","#     '220210_mad', '220210_range', '220210_var',\n","\n","#     'apsiii', 'age', 'gender', 'SOFA', 'vasopressin', 'ventilation'\n","# ]]\n","\n","# # Extract the label column\n","# labels = final_df_cleaned['label']\n","\n","# # 2. Split data into training and test sets\n","# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n","\n","# # 3. Standardize the data (optional step)\n","# scaler = StandardScaler()\n","# X_train_scaled = scaler.fit_transform(X_train)\n","# X_test_scaled = scaler.transform(X_test)\n","\n","# # Define base models (base learners), selecting from models\n","# models = {\n","#     \"Logistic Regression\": LogisticRegression(),\n","#     #\"MLP Classifier\": MLPClassifier(max_iter=1000),\n","#     #\"Naive Bayes\": GaussianNB(),\n","#     \"Random Forest\": RandomForestClassifier(n_estimators=100),\n","#     \"Decision Tree\": DecisionTreeClassifier(),\n","#     \"SVM\": SVC(probability=True),\n","#     #\"KNN\": KNeighborsClassifier(n_neighbors=5),\n","#     \"GBDT\": GradientBoostingClassifier(),  # GBDT model\n","#     \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),  # XGBoost model\n","#     \"AdaBoost\": AdaBoostClassifier(n_estimators=100)  # AdaBoost model\n","# }\n","\n","# # Convert the models dictionary to a list of (name, model) pairs for base learners\n","# base_learners = [(name, model) for name, model in models.items()]\n","\n","# # Define the meta-learner (secondary learner), using Logistic Regression as the meta-model\n","# stacking_model = StackingClassifier(\n","#     estimators=base_learners,\n","#     final_estimator=LogisticRegression(),  # Meta-learner, can be replaced with another model\n","#     cv=None\n","# )\n","\n","# # Train the Stacking model\n","# stacking_model.fit(X_train_scaled, y_train)\n","\n","# # Predict and evaluate the Stacking model\n","# y_pred = stacking_model.predict(X_test_scaled)\n","# y_pred_proba = stacking_model.predict_proba(X_test_scaled)[:, 1]\n","\n","# # Calculate AUROC\n","# auc = roc_auc_score(y_test, y_pred_proba)\n","\n","# # Output the model accuracy\n","# accuracy = accuracy_score(y_test, y_pred)\n","# print(f\"Stacking model accuracy: {accuracy:.4f}\")\n","# print(f\"Stacking model AUROC: {auc:.4f}\")\n","\n","# # Output detailed classification report\n","# print(classification_report(y_test, y_pred))\n"]},{"cell_type":"code","source":[],"metadata":{"id":"LByGF8x3Nn3Z"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}